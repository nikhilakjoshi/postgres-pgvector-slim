# Caching Module Requirements

## Objective

Build a caching module that reduces redundant LLM calls by storing and reusing answers to previously asked questions.
The module should intercept every user query, check for a similar cached entry, and return the cached answer if a match is found above a similarity threshold. Only if no match is found should the system make a fresh LLM call.

---

## Data Source

- **Messages Table** (Postgres, JSONB columns):

  - Contains both **user messages** and **assistant messages**.
  - Each user question may have:

    - Original user query.
    - Rephrased query (generated by a lightweight LLM before similarity check and retrieval).
    - Assistant’s final answer.

---

## Steps to Build

### 1. Data Extraction

- Create a **database view** that exposes:

  - `user_id` (optional if multi-user system).
  - `original_question`.
  - `rephrased_question` (used for embeddings & similarity search).
  - `answer` (assistant’s response).

This ensures all necessary pairs of `(rephrased_question → answer)` are available in a clean format.

---

### 2. Embeddings Store

- Generate embeddings for **all rephrased questions** in the view.
- Store them in a dedicated table:

  - `id` (PK).
  - `rephrased_question`.
  - `embedding` (vector type, e.g., pgvector).
  - `answer_reference` (link back to the cached answer).

Ensure background jobs periodically:

- Pick up **new entries** from the view.
- Generate embeddings.
- Insert them into the embeddings table.

---

### 3. Query Flow

When a **new question** is asked:

1. Pass it through the same **rephrasing LLM** to generate a normalized query.
2. Generate embeddings for the rephrased query.
3. Perform a **vector similarity search** against the embeddings table.

   - Use cosine similarity or inner product.
   - Define a configurable **similarity threshold** (e.g., 0.85).

4. If match **≥ threshold**:

   - Fetch the corresponding answer from cache.
   - Return it directly to the user.

5. If **no match**:

   - Proceed with normal flow (LLM + retrieval).
   - Cache the new `(rephrased_question, answer)` pair.

---

### 4. Configurations

- **Similarity Threshold** (default: 0.85).
- **Cache TTL** (optional – to expire old Q/As).
- **Max Cache Size** (optional – to avoid unbounded growth).

---

### 5. Interfaces

- **Cache Lookup API**: Given a rephrased question, return cached answer if match found.
- **Cache Update API**: Insert new Q/A pairs into embeddings table after LLM completion.

---

### 6. Monitoring & Metrics

- Track:

  - **Cache Hit Rate** (% queries answered without LLM).
  - **Average Latency Reduction** (compared to LLM calls).
  - **Cache Growth Over Time**.

---

### 7. Future Enhancements

- Add **semantic clustering** to group related questions for broader matching.
- Add **feedback signals** (user upvotes/downvotes) to adjust threshold dynamically.
- Multi-tenant support if needed (segregating cache per user or per team).

---

✅ Deliverable: A caching module that plugs into the existing pipeline, checks cached answers before invoking LLM, and self-updates as new Q/As appear.

---

Do you want me to also draft the **SQL view definition** and **pgvector schema** for the embeddings store so Claude can work from exact DDLs, or should we keep it at requirement level?
